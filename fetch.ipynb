{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import requests\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from asyncio import Semaphore\n",
    "import numpy as np\n",
    "\n",
    "ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "\n",
    "from aiohttp_socks import ProxyConnector\n",
    "proxies = ['socks5://127.0.0.1:1984','socks5://127.0.0.1:1986']\n",
    "connectors = [ProxyConnector.from_url(p) for p in proxies] + [None]\n",
    "\n",
    "\n",
    "sesss = [aiohttp.ClientSession(headers={'User-Agent': ua},connector=c,trust_env=True) for c in connectors]\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"Experiment-HEP\", \"Phenomenology-HEP\", \"Experiment-Nucl\", \"Instrumentation\", \"Theory-Nucl\", \"Astrophysics\", \"Lattice\", \"Theory-HEP\", \"Other\", \"General Physics\", \"Computing\", \"Accelerators\", \"Data Analysis and Statistics\", \"Gravitation and Cosmology\", \"Quantum Physics\", \"Condensed Matter\", \"Math and Math Physics\"]\n",
    "\n",
    "document_types = [\"published\", \"article\", \"conference paper\", \"review\", \"introductory\", \"lectures\", \"book chapter\", \"note\", \"thesis\", \"book\", \"report\", \"proceedings\"]\n",
    "\n",
    "arxiv_categories = [\"hep-ex\", \"hep-ph\", \"nucl-ex\", \"physics.ins-det\", \"nucl-th\", \"hep-lat\", \"astro-ph.CO\", \"astro-ph.HE\", \"astro-ph.IM\", \"physics.data-an\", \"hep-th\", \"physics.acc-ph\", \"quant-ph\", \"physics.atom-ph\", \"gr-qc\", \"cs.LG\", \"astro-ph.SR\", \"astro-ph.GA\", \"physics.comp-ph\", \"stat.ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def async_retry(timeout: float = 1, backoff: float = 2, max_retry: int = 5):\n",
    "    def inner(func):\n",
    "        _timeout = timeout\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            timeout = _timeout\n",
    "            for i in range(max_retry):\n",
    "                try:\n",
    "                    return await func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    funcname = func.__name__\n",
    "                    print(f'\\033[33m Caught {e} in {funcname}, retrying in {timeout} seconds...\\033[0m')\n",
    "                    await asyncio.sleep(timeout)\n",
    "                timeout *= backoff\n",
    "            raise Exception('Too many retries')\n",
    "        return wrapper\n",
    "    return inner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(subject=None, document_type=None, arxiv_category=None):\n",
    "    \n",
    "    url = 'https://inspirehep.net/api/literature'\n",
    "    params = {\n",
    "        'sort': 'mostrecent',\n",
    "        'size':10,\n",
    "        'page':1,\n",
    "    }\n",
    "\n",
    "    if subject:\n",
    "        assert subject in subjects\n",
    "        params['subject'] = subject\n",
    "    if document_type:\n",
    "        assert document_type in document_types\n",
    "        params['doc_type'] = document_type\n",
    "    if arxiv_category:\n",
    "        assert arxiv_category in arxiv_categories\n",
    "        params['arxiv_categories'] = arxiv_category\n",
    "    \n",
    "    return url, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url, params = gen_params(subject='Experiment-HEP', document_type='thesis')\n",
    "# path = Path('/tmp/data/' + '_'.join([f'{k}={v}' for k,v in params.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../../../data/inspirehep')\n",
    "# path = Path('/tmp/data/inspirehep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_keys = ['references', 'referenced_authors_bais', 'authors', '$schema', 'external_system_identifiers', 'documents']\n",
    "\n",
    "def to(s:str):\n",
    "    def f(X):\n",
    "        return [x[s] for x in X if s in x]\n",
    "    return f\n",
    "\n",
    "xest = {\n",
    "    'keywords': to('value'),\n",
    "    'inspire_categories' : to('term'),\n",
    "    'supervisors': lambda X: [(x['full_name'], x['recid']) for x in X if 'full_name' in x and 'recid' in x],\n",
    "    'urls': to('value'),\n",
    "    'first_author': lambda X: (X.get('full_name', None), X.get('recid', None)),\n",
    "    'titles': to('title'),\n",
    "    'abstracts': to('value'),\n",
    "    'imprints': to('date'),\n",
    "    'report_numbers': to('value'),\n",
    "}\n",
    "\n",
    "def cleanse(data:dict):\n",
    "    for k in del_keys:\n",
    "        if k in data:\n",
    "            del data[k]\n",
    "    for k, f in xest.items():\n",
    "        if k in data:\n",
    "            data[k] = f(data[k])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(sess, url, params):\n",
    "    async with sess.get(url, params=params) as resp:\n",
    "        if resp.status == 400:\n",
    "            return None\n",
    "        assert resp.status == 200\n",
    "        return await resp.json()\n",
    "\n",
    "@async_retry()\n",
    "async def load_balanced_fetch(sesss:list[aiohttp.ClientSession], p, q:asyncio.Queue, counter:list, pbar:tqdm, sema):\n",
    "    async with sema:\n",
    "        url, params = p\n",
    "        idx = np.argmin(counter)\n",
    "        counter[idx] += 1\n",
    "        r = await fetch(sesss[idx], url, params)\n",
    "        counter[idx] -= 1\n",
    "        if r is None:\n",
    "            return None\n",
    "\n",
    "        page = params['page']\n",
    "        if page == 1:\n",
    "            total_pages = r['hits']['total']\n",
    "            n_pages = int(np.ceil(total_pages / params['size']))\n",
    "            dd = 0\n",
    "            for i in range(2, n_pages+1):\n",
    "                q.put_nowait((url, {**params, 'page': i}))\n",
    "                dd += 1\n",
    "            pbar.total += dd\n",
    "            pbar.refresh()\n",
    "    q.task_done()\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_all(sesss:list[aiohttp.ClientSession], q:asyncio.Queue, counter:list, sema:Semaphore):\n",
    "    tasks = set()\n",
    "    with tqdm(position=0, desc='requests', total=len(q._queue)) as pbar: # type: ignore\n",
    "        \n",
    "        for _ in range(64):\n",
    "            if q.empty():\n",
    "                break\n",
    "            p = q.get_nowait()\n",
    "            tasks.add(asyncio.create_task(load_balanced_fetch(sesss,p,q,counter,pbar,sema)))\n",
    "        while tasks:\n",
    "            finished, unfinished = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)\n",
    "            for future in finished:\n",
    "                tasks.remove(future)\n",
    "                if not q.empty():\n",
    "                    p = q.get_nowait()\n",
    "                    tasks.add(asyncio.create_task(load_balanced_fetch(sesss,p,q,counter,pbar,sema)))\n",
    "                data = await future\n",
    "                if data is None:\n",
    "                    continue\n",
    "                for d in data['hits']['hits']:\n",
    "                    if 'metadata' not in d:\n",
    "                        print(d.keys())\n",
    "                        continue\n",
    "                    meta = cleanse(d['metadata'])\n",
    "                    meta['id'] = d.get('id')\n",
    "                    yield meta\n",
    "                pbar.update()\n",
    "\n",
    "\n",
    "async def save_all(sesss:list[aiohttp.ClientSession], q:asyncio.Queue, sema:Semaphore, counter:list, path:Path, chunk_size=1000):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    buffer = []\n",
    "    i = 0\n",
    "# with tqdm(position=1) as pbar:\n",
    "    seen = set()\n",
    "    async for data in fetch_all(sesss, q, counter, sema):\n",
    "        _id = data.get('id')\n",
    "        if _id is None:\n",
    "            print(data, 'wtf?')\n",
    "            continue\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        buffer.append(data)\n",
    "        seen.add(_id)\n",
    "        # pbar.update()\n",
    "        if len(buffer) >= chunk_size:\n",
    "            with open(path / f'{i}.json.zst', 'wb') as f:\n",
    "                f.write(zstd.compress(json.dumps(buffer, separators=(',', ':')).encode('utf-8'), 22))\n",
    "            buffer = []\n",
    "            i += 1\n",
    "    if buffer:\n",
    "        with open(path / f'{i}.json.zst', 'wb') as f:\n",
    "            f.write(zstd.compress(json.dumps(buffer, separators=(',', ':')).encode('utf-8'), 22))\n",
    "        buffer = []\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"Experiment-HEP\", \"Phenomenology-HEP\", \"Experiment-Nucl\", \"Instrumentation\", \"Theory-Nucl\", \"Astrophysics\", \"Lattice\", \"Theory-HEP\", \"Other\", \"General Physics\", \"Computing\", \"Accelerators\", \"Data Analysis and Statistics\", \"Gravitation and Cosmology\", \"Quantum Physics\", \"Condensed Matter\", \"Math and Math Physics\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, _ = gen_params()\n",
    "q = asyncio.Queue()\n",
    "for s in subjects:\n",
    "    url, params = gen_params(subject=s, document_type='thesis')\n",
    "    params['earliest_date'] = '1500--1974'\n",
    "    q.put_nowait((url, params))\n",
    "    for y in range(1975, 2024):\n",
    "        url, params = gen_params(subject=s, document_type='thesis')\n",
    "        params['earliest_date'] = f'{y}--{y}'\n",
    "        q.put_nowait((url, params))\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests: 100%|██████████| 6421/6421 [13:45<00:00,  7.78it/s]  \n"
     ]
    }
   ],
   "source": [
    "await save_all(sesss, q, Semaphore(24), [0,0,0], path/'thesis', chunk_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, _ = gen_params()\n",
    "q = asyncio.Queue()\n",
    "for s in subjects:\n",
    "    url, params = gen_params(subject=s, document_type='published')\n",
    "    params['earliest_date'] = '1500--1974'\n",
    "    q.put_nowait((url, params))\n",
    "    for y in range(1975, 2024):\n",
    "        url, params = gen_params(subject=s, document_type='published')\n",
    "        params['earliest_date'] = f'{y}--{y}'\n",
    "        q.put_nowait((url, params))\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:   4%|▍         | 4430/104883 [06:18<18:23:05,  1.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Response payload is not completed in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:   4%|▍         | 4435/104883 [06:18<9:18:16,  3.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Response payload is not completed in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:   5%|▌         | 5335/104883 [08:48<38:26:28,  1.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Response payload is not completed in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:   5%|▌         | 5433/104883 [08:50<45:39, 36.30it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Response payload is not completed in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:  12%|█▏        | 13070/104883 [16:16<20:42:43,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:  23%|██▎       | 24380/104883 [31:55<10:51:56,  2.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:  24%|██▍       | 25389/104883 [33:28<12:06:16,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught [Errno 32] Broken pipe in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught [Errno 32] Broken pipe in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught [Errno 32] Broken pipe in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:  43%|████▎     | 45012/104883 [49:07<26:12:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n",
      "\u001b[33m Caught Server disconnected in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests:  51%|█████▏    | 53820/104883 [1:00:25<9:31:31,  1.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Caught Response payload is not completed in load_balanced_fetch, retrying in 1 seconds...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "requests: 100%|█████████▉| 104810/104883 [1:36:38<00:04, 18.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "await save_all(sesss, q, Semaphore(24), [0,0,0], path/'published', chunk_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
